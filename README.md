# Airflow + dbt + Snowflake (Postgres-backed) Demo ðŸ¦ŠðŸ±

A reproducible, stable local data orchestration template: Apache Airflow for scheduling, dbt for modeling, Postgres as Airflow metadata DB, and Snowflake as the warehouse. Includes one-command startup, health checks, regression validation, and cleanup helpers.

## Quick Start

Prereqs: Docker Desktop â‰¥ 4.x, GNU Make, bash, curl

1) Credentials (kept local, not committed)
- Create `airflow/.env` (you can copy from `airflow/.env.example`) and fill in Snowflake creds: account, user, password, role, warehouse, database, schema. These envs are passed into containers for dbt to use.

2) Start (choose one mode)
- `./launch.sh --init`      # one-time init + start
- `./launch.sh --rebuild`   # rebuild images (installs deps) + start
- `./launch.sh --upgrade`   # pull latest images and recreate containers
- `./launch.sh --fresh`     # nuke volumes and start clean (dangerous)
- When healthy, open: `http://localhost:8080`
  - User/pass: `airflow / airflow`

3) Validate
- Trigger and wait for all DAGs to succeed: `make validate`
- Or validate a subset: `make validate-daily` / `make validate-pipelines`

4) Clear red dots (historical failures)
- Keep run records, clear failed task instances: `make clear-failed`
- Remove failed runs (destructive): `make clear-failed-hard`

## Layout

```
./
â”œâ”€ airflow/                  # Airflow (DAGs, container requirements, .env)
â”‚  â”œâ”€ dags/
â”‚  â”‚  â”œâ”€ dbt_daily.py
â”‚  â”‚  â”œâ”€ dbt_pipeline_dag.py
â”‚  â”‚  â””â”€ dbt_layered_pipeline.py
â”‚  â”œâ”€ requirements.txt       # installed at container startup via _PIP_ADDITIONAL_REQUIREMENTS
â”‚  â””â”€ .env                   # Snowflake credentials (gitignored)
â”œâ”€ data_pipeline/            # dbt project root
â”‚  â”œâ”€ dbt_project.yml
â”‚  â”œâ”€ profiles.yml
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ bronze/
â”‚  â”‚  â”œâ”€ silver/
â”‚  â”‚  â””â”€ gold/
â”‚  â”œâ”€ snippets/              # copy-paste templates (relationships, sources)
â”‚  â””â”€ .gitignore
â”œâ”€ scripts/
â”‚  â”œâ”€ validate.sh            # trigger + wait for 3 DAGs
â”‚  â””â”€ clear_failed.sh        # clear failed instances / delete failed runs
â”œâ”€ docker-compose.yml        # Postgres + LocalExecutor root stack
â”œâ”€ launch.sh                 # one-click bootstrap + health + logs
â”œâ”€ Makefile                  # handy targets (validate/clear/health)
â””â”€ README.md
```

## Stack & Config

- Airflow 2.9.3 (`apache/airflow:2.9.3-python3.11`)
  - Executor: LocalExecutor
  - Metadata DB: Postgres (`postgres:15-alpine`)
  - Healthcheck: `airflow db check`
- dbt-core 1.10 + dbt-snowflake 1.10 (installed at container start)
- Mounts:
  - `./airflow/dags -> /opt/airflow/dags`
  - `./data_pipeline -> /opt/airflow/dbt`

Compose highlights:
- `airflow-init` runs `airflow db init`, creates `airflow/airflow` user, and a `dbt` pool (size 1)
- Scheduler/Webserver depend on Postgres healthy + init completed

## Runtime Conventions (stability)

- All dbt tasks use Airflow pool `dbt` (size 1) to serialize dbt CLI; avoids `target/` and `dbt_packages/` races
- DAGs use `max_active_runs=1` and one retry by default to reduce flakiness
- No deletion of `dbt_packages/target` in tasks; only `dbt deps` to keep deps consistent

## Common Ops

- Check health
  - `docker compose ps`
  - `curl -fsS http://localhost:8080/health`
- Tail logs
  - `docker compose logs webserver --tail 100 -f`
  - `docker compose logs scheduler --tail 100 -f`
- Trigger / inspect DAGs
  - `docker compose exec -T webserver airflow dags list`
  - `docker compose exec -T webserver airflow dags trigger dbt_daily`
  - `docker compose exec -T webserver airflow dags list-runs -d dbt_daily -o table`

## Email / Notifications (builtâ€‘in Mailpit)

- This stack includes a local SMTP sink using Mailpit for easy testing.
  - Web UI: `http://localhost:8025`
  - SMTP host/port (preconfigured): `mailpit:1025` (no auth, no TLS)
- Smoke test DAG: `smtp_smoke`
  - Trigger in UI or run: `docker compose exec -T webserver airflow dags trigger smtp_smoke`
  - Open Mailpit UI to see the test email in the inbox.
- Switching to a real SMTP provider (e.g., Gmail/Mailtrap):
  - Edit `airflow/.env` SMTP variables (see `airflow/.env.example`) and restart services:
    - `docker compose restart webserver scheduler` (or `up -d --force-recreate`)
  - For Gmail, enable 2FA and use an App Password; for development, Mailtrap is recommended.

## Pipeline Differences & Run Order

Whatâ€™s special in this repo:
- TaskGroup helpers for dbt run/test with backfill vars and pools (`airflow/dags/lib/`)
- Layered pipeline enforces quality gates between layers
- Datasets publish/subscribe from gold completion
- Optional Great Expectations context is mounted and runnable out-of-the-box

Run order (layered example):

```
dbt_deps
  â†’ [bronze.run] â†’ [bronze.test]
  â†’ [silver.run] â†’ [silver.test]
  â†’ [gold.run]   â†’ [gold.test] â†’ publishes Dataset: dbt://gold/fct_orders
```

dbt_daily (smoke) is simpler:

```
dbt_deps â†’ dbt_run â†’ dbt_test â†’ publishes Dataset
```

## Templates (sources/tests)

- See `data_pipeline/snippets/` for new-style relationships (with `arguments`) and a standard source template. Copy and replace placeholders.

## Secrets

- `.env` is gitignored â€” keep credentials out of the repo
- For production, bake deps into an image and use a secret manager (Vault/KMS/Secrets Manager)

## Troubleshooting

- UI red dots indicate historical failures â€” use `make clear-failed` to clean up
- If containers restart repeatedly: `./launch.sh --fresh`
- If Docker Desktop misbehaves: restart Docker, then rerun `./launch.sh`
