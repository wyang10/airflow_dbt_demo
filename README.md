# Airflow + dbt + Snowflake (Postgresâ€‘backed) Demo ğŸ¦ŠğŸ±

Layered pipeline with explicit quality gates and reusable TaskGroups.

Run Order (Layered Pipeline)
- dbt_deps â†’ [bronze.run] â†’ [bronze.test] â†’ [silver.run] â†’ [silver.test] â†’ [gold.run] â†’ [gold.test] â†’ publish Dataset `dbt://gold/fct_orders`

TaskGroup Pattern
- `dbt_run_group(selector, env, project_dir, pool)`: wraps `dbt run` with backfill vars `{start_date,end_date}`; XCom disabled; pool defaults to `dbt`.
- `dbt_test_group(selector, env, project_dir, pool)`: wraps `dbt test` with the same vars; acts as the quality gate for the previous run group.
- Benefits: consistent structure, minimal boilerplate, easy layering by chaining groups between domains.


A reproducible, stable local data orchestration template: Apache Airflow for scheduling, dbt for modeling, Postgres as Airflow metadata DB, and Snowflake as the warehouse. Includes one-command startup, health checks, regression validation, and cleanup helpers.

## Quick Start

Prereqs: Docker Desktop â‰¥ 4.x, GNU Make, bash, curl

1) Credentials (kept local, not committed)
- Create `airflow/.env` (you can copy from `airflow/.env.example`) and fill in Snowflake creds: account, user, password, role, warehouse, database, schema. These envs are passed into containers for dbt to use.

2) Start (choose one mode)
- `./launch.sh --init`      # one-time init + start
- `./launch.sh --rebuild`   # rebuild images (installs deps) + start
- `./launch.sh --upgrade`   # pull latest images and recreate containers
- `./launch.sh --fresh`     # nuke volumes and start clean (dangerous)
- When healthy, open: `http://localhost:8080`
  - User/pass: `airflow / airflow`

3) Validate
- Trigger and wait for all DAGs to succeed: `make validate`
- Or validate a subset: `make validate-daily` / `make validate-pipelines`

4) Clear red dots (historical failures)
- Keep run records, clear failed task instances: `make clear-failed`
- Remove failed runs (destructive): `make clear-failed-hard`

## Layout

```
./
â”œâ”€ airflow/                  # Airflow (DAGs, container requirements, .env)
â”‚  â”œâ”€ dags/
â”‚  â”‚  â”œâ”€ dbt_daily.py
â”‚  â”‚  â”œâ”€ dbt_pipeline_dag.py
â”‚  â”‚  â””â”€ dbt_layered_pipeline.py
â”‚  â”œâ”€ requirements.txt       # installed at container startup via _PIP_ADDITIONAL_REQUIREMENTS
â”‚  â””â”€ .env                   # Snowflake credentials (gitignored)
â”œâ”€ data_pipeline/            # dbt project root
â”‚  â”œâ”€ dbt_project.yml
â”‚  â”œâ”€ profiles.yml
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ bronze/
â”‚  â”‚  â”œâ”€ silver/
â”‚  â”‚  â””â”€ gold/
â”‚  â”œâ”€ snippets/              # copy-paste templates (relationships, sources)
â”‚  â””â”€ .gitignore
â”œâ”€ scripts/
â”‚  â”œâ”€ validate.sh            # trigger + wait for 3 DAGs
â”‚  â””â”€ clear_failed.sh        # clear failed instances / delete failed runs
â”œâ”€ docker-compose.yml        # Postgres + LocalExecutor root stack
â”œâ”€ launch.sh                 # one-click bootstrap + health + logs
â”œâ”€ Makefile                  # handy targets (validate/clear/health)
â””â”€ README.md
```

## Stack & Config

- Airflow 2.9.3 (`apache/airflow:2.9.3-python3.11`)
  - Executor: LocalExecutor
  - Metadata DB: Postgres (`postgres:15-alpine`)
  - Healthcheck: `airflow db check`
- dbt-core 1.10 + dbt-snowflake 1.10 (installed at container start)
- Mounts:
  - `./airflow/dags -> /opt/airflow/dags`
  - `./data_pipeline -> /opt/airflow/dbt`

Compose highlights:
- `airflow-init` runs `airflow db init`, creates `airflow/airflow` user, and a `dbt` pool (size 1)
- Scheduler/Webserver depend on Postgres healthy + init completed

## Runtime Conventions (stability)

- All dbt tasks use Airflow pool `dbt` (size 1) to serialize dbt CLI; avoids `target/` and `dbt_packages/` races
- DAGs use `max_active_runs=1` and one retry by default to reduce flakiness
- No deletion of `dbt_packages/target` in tasks; only `dbt deps` to keep deps consistent

## Common Ops

- Check health
  - `docker compose ps`
  - `curl -fsS http://localhost:8080/health`
- Tail logs
  - `docker compose logs webserver --tail 100 -f`
  - `docker compose logs scheduler --tail 100 -f`
- Trigger / inspect DAGs
  - `docker compose exec -T webserver airflow dags list`
  - `docker compose exec -T webserver airflow dags trigger dbt_daily`
  - `docker compose exec -T webserver airflow dags list-runs -d dbt_daily -o table`

## Email / Notifications (builtâ€‘in Mailpit)

- This stack includes a local SMTP sink using Mailpit for easy testing.
  - Web UI: `http://localhost:8025`
  - SMTP host/port (preconfigured): `mailpit:1025` (no auth, no TLS)
- Smoke test DAG: `smtp_smoke`
  - Trigger in UI or run: `docker compose exec -T webserver airflow dags trigger smtp_smoke`
  - Open Mailpit UI to see the test email in the inbox.
- Default connection used by DAG: `smtp_mailpit` (auto-provisioned via env `AIRFLOW_CONN_SMTP_MAILPIT`).
- Switching to a real SMTP provider (e.g., Gmail/Mailtrap):
  - Create a new Airflow Connection `smtp_gmail` via UI (Admin â†’ Connections â†’ +):
    - Conn Id: `smtp_gmail`
    - Conn Type: `smtp`
    - Host: `smtp.gmail.com`
    - Port: `587`
    - Login: `<your Gmail address>`
    - Password: `<Gmail App Password>`
    - Extra: `{ "starttls": true }`
  - æˆ–ä½¿ç”¨ CLIï¼š
    - `docker compose exec -T webserver airflow connections add smtp_gmail \`
      `--conn-type smtp --conn-host smtp.gmail.com --conn-port 587 \`
      `--conn-login YOUR@GMAIL.COM --conn-password 'APP_PASSWORD' \`
      `--conn-extra '{"starttls": true}'`
  - å°† `smtp_smoke` çš„ `conn_id` æ”¹ä¸º `smtp_gmail`ï¼ˆå½“å‰é»˜è®¤å·²ç”¨ `smtp_mailpit`ï¼‰ã€‚

Tip: `airflow/.env` é‡ŒåŒ…å«ä¸€ä¸ªç©ºå¯†ç çš„æ¨¡æ¿å˜é‡ï¼ˆæ³¨é‡Šè¡Œï¼‰ï¼š
`AIRFLOW_CONN_SMTP_GMAIL=smtp://your.name@gmail.com:@smtp.gmail.com:587?starttls=true`
éœ€è¦çœŸå®å‘ä¿¡æ—¶ï¼Œå»ºè®®ç›´æ¥ç”¨ Airflow UI/CLI å»ºç«‹è¿æ¥å¹¶å¡«å…¥ App Passwordã€‚

## Great Expectations Data Docs

- å·²é›†æˆæœ¬åœ° Data Docsï¼Œå¯é€šè¿‡ Nginx æš´éœ²ä¸ºé™æ€ç«™ç‚¹ï¼š
  - æ‰“å¼€ `http://localhost:8081`
- è´¨é‡æ£€æŸ¥ DAGï¼š`quality_checks`
  - è¿è¡Œåè‡ªåŠ¨ç”Ÿæˆ/æ›´æ–° Data Docsï¼ˆ`UpdateDataDocsAction`ï¼‰
  - åœ¨ Airflow ä»»åŠ¡é¡µé¢çš„ Extra Links ä¸­ï¼Œç‚¹å‡» â€œGreat Expectations Data Docsâ€ å³å¯è·³è½¬ï¼ˆå·²è‡ªåŠ¨é‡å†™ä¸º `http://localhost:8081/...`ï¼‰

## Pipeline Differences & Run Order

Whatâ€™s special in this repo:
- TaskGroup helpers for dbt run/test with backfill vars and pools (`airflow/dags/lib/`)
- Layered pipeline enforces quality gates between layers
- Datasets publish/subscribe from gold completion
- Optional Great Expectations context is mounted and runnable out-of-the-box

Run order (layered example):

```
dbt_deps
  â†’ [bronze.run] â†’ [bronze.test]
  â†’ [silver.run] â†’ [silver.test]
  â†’ [gold.run]   â†’ [gold.test] â†’ publishes Dataset: dbt://gold/fct_orders
```

dbt_daily (smoke) is simpler:

```
dbt_deps â†’ dbt_run â†’ dbt_test â†’ publishes Dataset
```

## Templates (sources/tests)

- See `data_pipeline/snippets/` for new-style relationships (with `arguments`) and a standard source template. Copy and replace placeholders.

## Secrets

- `.env` is gitignored â€” keep credentials out of the repo
- For production, bake deps into an image and use a secret manager (Vault/KMS/Secrets Manager)

## Troubleshooting

- UI red dots indicate historical failures â€” use `make clear-failed` to clean up
- If containers restart repeatedly: `./launch.sh --fresh`
- If Docker Desktop misbehaves: restart Docker, then rerun `./launch.sh`
