version: "3.8"

# NOTE:
# - This file is a legacy single-folder compose (SQLite + SequentialExecutor).
# - The main stack lives at repo root: `docker-compose.yml` (Postgres + LocalExecutor).
# - Do not hardcode secrets here; use env vars or an env file.

services:
  airflow-init:
    build: .
    image: local/airflow-dbt:latest
    command: bash -lc "airflow db init && airflow users create \
      --username admin --password admin --firstname Admin --lastname User \
      --role Admin --email admin@example.com"
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY:-}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY:-dev-change-me}

      DBT_PROFILES_DIR: /opt/airflow/dbt
    volumes:
      - airflow_home:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ../data_pipeline:/opt/airflow/dbt      # ← 对齐到 data_pipeline
    restart: "no"

  webserver:
    image: local/airflow-dbt:latest
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: webserver
    ports:
      - "8080:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY:-dev-change-me}
      DBT_PROFILES_DIR: /opt/airflow/dbt                     # <<<
    volumes:
      - airflow_home:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ../data_pipeline:/opt/airflow/dbt      # ← 同步                           # <<<
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-lc", "airflow db check"]
      interval: 30s
      timeout: 10s
      retries: 5

  scheduler:
    image: local/airflow-dbt:latest
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
      DBT_PROFILES_DIR: /opt/airflow/dbt                     # <<<
    volumes:
      - airflow_home:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ../data_pipeline:/opt/airflow/dbt      # ← 同步                             # <<<
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-lc", "airflow db check"]
      interval: 30s
      timeout: 10s
      retries: 5

volumes:
  airflow_home: {}
